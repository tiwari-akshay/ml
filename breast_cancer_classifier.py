# -*- coding: utf-8 -*-
"""Breast Cancer Classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1akINdhP7qecZqCAEs0KunW6Hraq39F5T

# 1. Exploratory Data Analysis

### 1.1 Understanding the data
"""

# Commented out IPython magic to ensure Python compatibility.
# importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

# reading data into the dataframe
df = pd.read_csv('/content/Cancer Data/data.csv')

# displaying first five rows
df.head()

# concise summary of dataframe
df.info()

# column names
df.columns

# checking for null values
df.isnull().sum()

"""The whole column 'Unamed: 32' has NaN values."""

# dropping 'Unnamed: 32' column.
df.drop("Unnamed: 32", axis=1, inplace=True)

# dropping id column
df.drop('id',axis=1, inplace=True)

# descriptive statistics of data
df.describe()

"""### 1.2. Data Visualizations"""

# countplot
plt.figure(figsize = (8,7))
sns.countplot(x="diagnosis", data=df, palette='magma')

# heatmap
plt.figure(figsize=(20,18))
# Select only numerical columns for correlation calculation
numerical_df = df.select_dtypes(include=['number'])
sns.heatmap(numerical_df.corr(), annot=True,linewidths=.5, cmap="Purples")

"""From the heatmap, we can observe from the heatmaps that there are many negative correlations in this dataset."""

df.columns

"""The mean, standard error and "worst" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius."""

# Getting Mean Columns with diagnosis
m_col = ['diagnosis','radius_mean', 'texture_mean', 'perimeter_mean',
       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',
       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']

# Getting Se Columns with diagnosis
s_col = ['diagnosis','radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',
       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',
       'fractal_dimension_se']

# Getting Worst column with diagnosis
w_col = ['diagnosis','radius_worst', 'texture_worst',
       'perimeter_worst', 'area_worst', 'smoothness_worst',
       'compactness_worst', 'concavity_worst', 'concave points_worst',
       'symmetry_worst', 'fractal_dimension_worst']

"""## For Mean Columns"""

# pairplot for mean columns
sns.pairplot(df[m_col],hue = 'diagnosis', palette='Blues')

"""## For SE Columns"""

# pairplot for se columns
sns.pairplot(df[s_col],hue = 'diagnosis', palette='Greens')

"""## For Worst Columns"""

# pairplot for worst columns
sns.pairplot(df[w_col],hue = 'diagnosis', palette='Oranges')

"""# 2. Data Preprocessing and Building Models

## 2.1 Data Preprocessing
"""

# counts of unique rows in the 'diagnosis' column
df['diagnosis'].value_counts()

# mapping categorical values to numerical values
df['diagnosis']=df['diagnosis'].map({'B':0,'M':1})

df['diagnosis'].value_counts()

"""## 2.2 Splitting the data into train and test"""

from sklearn.model_selection import train_test_split

# splitting data
X_train, X_test, y_train, y_test = train_test_split(
                df.drop('diagnosis', axis=1),
                df['diagnosis'],
                test_size=0.2,
                random_state=42)

print("Shape of training set:", X_train.shape)
print("Shape of test set:", X_test.shape)

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
X_train = ss.fit_transform(X_train)
X_test = ss.fit_transform(X_test)

"""StandardScaler standardizes a feature by subtracting the mean and then scaling to unit variance.(Unit variance means dividing all the values by the standard deviation.)

## 2.3 Classification Models

### 2.3.1 Logistic Regression
"""

from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression()
logreg.fit(X_train, y_train)
predictions1 = logreg.predict(X_test)

from sklearn.metrics import confusion_matrix, classification_report

print("Confusion Matrix: \n", confusion_matrix(y_test, predictions1))
print('\n')
print(classification_report(y_test, predictions1))

from sklearn.metrics import accuracy_score

logreg_acc = accuracy_score(y_test, predictions1)
print("Accuracy of the Logistic Regression Model is: ", logreg_acc)

"""### 2.3.3 Random Forests"""

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(n_estimators=300)
rfc.fit(X_train, y_train)
predictions4 = rfc.predict(X_test)

print("Confusion Matrix: \n", confusion_matrix(y_test, predictions4))
print("\n")
print(classification_report(y_test, predictions4))

rfc_acc = accuracy_score(y_test, predictions4)
print("Accuracy of Random Forests Model is: ", rfc_acc)

"""# 3. Final Results"""

print(logreg_acc)
print(rfc_acc)

"""The accuracy of Logistic Regression Model is 98.24%


The accuracy of Random Forest Model is 96.49%
"""

plt.figure(figsize=(12,6))
model_acc = [logreg_acc,rfc_acc]
model_name = ['LogisticRegression', 'RandomForests']
sns.barplot(x= model_acc, y=model_name, palette='magma')

