---
jupyter:
  colab:
  kernelspec:
    display_name: Python 3
    name: python3
  language_info:
    name: python
  nbformat: 4
  nbformat_minor: 0
---

::: {.cell .markdown id="lEVi09KeuYXF"}
# 1. Exploratory Data Analysis {#1-exploratory-data-analysis}
:::

::: {.cell .markdown id="DipQ7Tf-up0N"}
### 1.1 Understanding the data {#11-understanding-the-data}
:::

::: {.cell .code execution_count="1" id="zHnID2k5tLRT"}
``` python
# importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
```
:::

::: {.cell .code execution_count="2" id="ai1X0QENtrtN"}
``` python
# reading data into the dataframe
df = pd.read_csv('/content/Cancer Data/data.csv')
```
:::

::: {.cell .code execution_count="3" colab="{\"base_uri\":\"https://localhost:8080/\",\"height\":273}" id="ofjuvAJ7tyPG" outputId="89967ed6-6559-47d6-86c2-7f5f5b0b8455"}
``` python
# displaying first five rows
df.head()
```

::: {.output .execute_result execution_count="3"}
``` json
{"type":"dataframe","variable_name":"df"}
```
:::
:::

::: {.cell .code execution_count="4" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="_ZWIv2J0t1-0" outputId="667a8516-50ca-4d03-88f4-fa43e5feb9ce"}
``` python
# concise summary of dataframe
df.info()
```

::: {.output .stream .stdout}
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 569 entries, 0 to 568
    Data columns (total 33 columns):
     #   Column                   Non-Null Count  Dtype  
    ---  ------                   --------------  -----  
     0   id                       569 non-null    int64  
     1   diagnosis                569 non-null    object 
     2   radius_mean              569 non-null    float64
     3   texture_mean             569 non-null    float64
     4   perimeter_mean           569 non-null    float64
     5   area_mean                569 non-null    float64
     6   smoothness_mean          569 non-null    float64
     7   compactness_mean         569 non-null    float64
     8   concavity_mean           569 non-null    float64
     9   concave points_mean      569 non-null    float64
     10  symmetry_mean            569 non-null    float64
     11  fractal_dimension_mean   569 non-null    float64
     12  radius_se                569 non-null    float64
     13  texture_se               569 non-null    float64
     14  perimeter_se             569 non-null    float64
     15  area_se                  569 non-null    float64
     16  smoothness_se            569 non-null    float64
     17  compactness_se           569 non-null    float64
     18  concavity_se             569 non-null    float64
     19  concave points_se        569 non-null    float64
     20  symmetry_se              569 non-null    float64
     21  fractal_dimension_se     569 non-null    float64
     22  radius_worst             569 non-null    float64
     23  texture_worst            569 non-null    float64
     24  perimeter_worst          569 non-null    float64
     25  area_worst               569 non-null    float64
     26  smoothness_worst         569 non-null    float64
     27  compactness_worst        569 non-null    float64
     28  concavity_worst          569 non-null    float64
     29  concave points_worst     569 non-null    float64
     30  symmetry_worst           569 non-null    float64
     31  fractal_dimension_worst  569 non-null    float64
     32  Unnamed: 32              0 non-null      float64
    dtypes: float64(31), int64(1), object(1)
    memory usage: 146.8+ KB
:::
:::

::: {.cell .code execution_count="5" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="D3MfEi0Gt5DO" outputId="3cad9497-5b6f-4ebe-982b-ce88e9966c9a"}
``` python
# column names
df.columns
```

::: {.output .execute_result execution_count="5"}
    Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',
           'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',
           'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',
           'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',
           'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',
           'fractal_dimension_se', 'radius_worst', 'texture_worst',
           'perimeter_worst', 'area_worst', 'smoothness_worst',
           'compactness_worst', 'concavity_worst', 'concave points_worst',
           'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32'],
          dtype='object')
:::
:::

::: {.cell .code execution_count="6" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="4II1Saeyt8YT" outputId="649237b3-95a6-4b57-e3f9-478185dd659a"}
``` python
# checking for null values
df.isnull().sum()
```

::: {.output .execute_result execution_count="6"}
    id                           0
    diagnosis                    0
    radius_mean                  0
    texture_mean                 0
    perimeter_mean               0
    area_mean                    0
    smoothness_mean              0
    compactness_mean             0
    concavity_mean               0
    concave points_mean          0
    symmetry_mean                0
    fractal_dimension_mean       0
    radius_se                    0
    texture_se                   0
    perimeter_se                 0
    area_se                      0
    smoothness_se                0
    compactness_se               0
    concavity_se                 0
    concave points_se            0
    symmetry_se                  0
    fractal_dimension_se         0
    radius_worst                 0
    texture_worst                0
    perimeter_worst              0
    area_worst                   0
    smoothness_worst             0
    compactness_worst            0
    concavity_worst              0
    concave points_worst         0
    symmetry_worst               0
    fractal_dimension_worst      0
    Unnamed: 32                569
    dtype: int64
:::
:::

::: {.cell .markdown id="jBLm-ak4uAKT"}
The whole column \'Unamed: 32\' has NaN values.
:::

::: {.cell .code execution_count="7" id="iNwfwNnYt-5S"}
``` python
# dropping 'Unnamed: 32' column.
df.drop("Unnamed: 32", axis=1, inplace=True)
```
:::

::: {.cell .code execution_count="8" id="eGQnLr30uOFs"}
``` python
# dropping id column
df.drop('id',axis=1, inplace=True)
```
:::

::: {.cell .code execution_count="9" colab="{\"base_uri\":\"https://localhost:8080/\",\"height\":367}" id="fhSPT4xhuQyh" outputId="4a38823b-9969-478e-fd8d-417a0a7beeac"}
``` python
# descriptive statistics of data
df.describe()
```

::: {.output .execute_result execution_count="9"}
``` json
{"type":"dataframe"}
```
:::
:::

::: {.cell .markdown id="LxuszoNGu1Ui"}
### 1.2. Data Visualizations {#12-data-visualizations}
:::

::: {.cell .code execution_count="10" colab="{\"base_uri\":\"https://localhost:8080/\",\"height\":743}" id="GPYXIbnruS9U" outputId="abb44372-e2c7-4618-ac57-16ee7dd15eb0"}
``` python
# countplot
plt.figure(figsize = (8,7))
sns.countplot(x="diagnosis", data=df, palette='magma')
```

::: {.output .stream .stderr}
    <ipython-input-10-66acbccb5508>:3: FutureWarning: 

    Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

      sns.countplot(x="diagnosis", data=df, palette='magma')
:::

::: {.output .execute_result execution_count="10"}
    <Axes: xlabel='diagnosis', ylabel='count'>
:::

::: {.output .display_data}
![](vertopal_a728d848c81c4bdb9ee2702eb782ef08/d8a7a252d79b47beae340f21e40dcbed3e86992f.png)
:::
:::

::: {.cell .code execution_count="13" colab="{\"base_uri\":\"https://localhost:8080/\",\"height\":871}" id="B9cdCHx6u5S0" outputId="6c5a8239-d58f-4474-cc26-eef1cec3f91c"}
``` python
# heatmap
plt.figure(figsize=(20,18))
# Select only numerical columns for correlation calculation
numerical_df = df.select_dtypes(include=['number'])
sns.heatmap(numerical_df.corr(), annot=True,linewidths=.5, cmap="Purples")
```

::: {.output .execute_result execution_count="13"}
    <Axes: >
:::

::: {.output .display_data}
![](vertopal_a728d848c81c4bdb9ee2702eb782ef08/64d508323bd20247ce02f814c1c8d834ac5047dc.png)
:::
:::

::: {.cell .markdown id="h8g_FJW5wz-M"}
From the heatmap, we can observe from the heatmaps that there are many
negative correlations in this dataset.
:::

::: {.cell .code execution_count="14" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="GycPO--tu7ge" outputId="89054a2a-01b1-4781-f38a-463c32c0e35a"}
``` python
df.columns
```

::: {.output .execute_result execution_count="14"}
    Index(['diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',
           'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',
           'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',
           'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',
           'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',
           'fractal_dimension_se', 'radius_worst', 'texture_worst',
           'perimeter_worst', 'area_worst', 'smoothness_worst',
           'compactness_worst', 'concavity_worst', 'concave points_worst',
           'symmetry_worst', 'fractal_dimension_worst'],
          dtype='object')
:::
:::

::: {.cell .markdown id="Vb07TqSOxIia"}
The mean, standard error and \"worst\" or largest (mean of the three
largest values) of these features were computed for each image,
resulting in 30 features. For instance, field 3 is Mean Radius, field 13
is Radius SE, field 23 is Worst Radius.
:::

::: {.cell .code execution_count="15" id="UAYCU5AGxAbB"}
``` python
# Getting Mean Columns with diagnosis
m_col = ['diagnosis','radius_mean', 'texture_mean', 'perimeter_mean',
       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',
       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']

# Getting Se Columns with diagnosis
s_col = ['diagnosis','radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',
       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',
       'fractal_dimension_se']

# Getting Worst column with diagnosis
w_col = ['diagnosis','radius_worst', 'texture_worst',
       'perimeter_worst', 'area_worst', 'smoothness_worst',
       'compactness_worst', 'concavity_worst', 'concave points_worst',
       'symmetry_worst', 'fractal_dimension_worst']
```
:::

::: {.cell .markdown id="oCzfMRkYxS4U"}
## For Mean Columns
:::

::: {.cell .code execution_count="16" colab="{\"base_uri\":\"https://localhost:8080/\",\"height\":857}" id="OMw0v-ynxM9a" outputId="967f5245-cb8f-44f7-d49e-bb043d69e207"}
``` python
# pairplot for mean columns
sns.pairplot(df[m_col],hue = 'diagnosis', palette='Blues')
```

::: {.output .execute_result execution_count="16"}
    <seaborn.axisgrid.PairGrid at 0x7a1fc47a0490>
:::

::: {.output .display_data}
![](vertopal_a728d848c81c4bdb9ee2702eb782ef08/2c82e3c321c74be89b70513504ae94b8702e9a53.png)
:::
:::

::: {.cell .markdown id="VYlF57tuxT9t"}
## For SE Columns
:::

::: {.cell .code execution_count="17" colab="{\"base_uri\":\"https://localhost:8080/\",\"height\":857}" id="1n6l3DBExc2A" outputId="cb0063f6-4dec-41ea-a992-e8a7dec1c3dc"}
``` python
# pairplot for se columns
sns.pairplot(df[s_col],hue = 'diagnosis', palette='Greens')
```

::: {.output .execute_result execution_count="17"}
    <seaborn.axisgrid.PairGrid at 0x7a1f79209750>
:::

::: {.output .display_data}
![](vertopal_a728d848c81c4bdb9ee2702eb782ef08/8494781eaffc4cd2f857e9f7d549295a446d5020.png)
:::
:::

::: {.cell .markdown id="y81fhvkJxUVr"}
## For Worst Columns
:::

::: {.cell .code execution_count="18" colab="{\"base_uri\":\"https://localhost:8080/\",\"height\":857}" id="7RNGWTfaxdcz" outputId="eeb9c9c7-0e1d-4ccc-eedd-d7848ebfdf84"}
``` python
# pairplot for worst columns
sns.pairplot(df[w_col],hue = 'diagnosis', palette='Oranges')
```

::: {.output .execute_result execution_count="18"}
    <seaborn.axisgrid.PairGrid at 0x7a1f6a1c8370>
:::

::: {.output .display_data}
![](vertopal_a728d848c81c4bdb9ee2702eb782ef08/9448e5a6771b4f07dff59f2a724df0018681d294.png)
:::
:::

::: {.cell .markdown id="rIuhbEQKxno1"}
# 2. Data Preprocessing and Building Models {#2-data-preprocessing-and-building-models}
:::

::: {.cell .markdown id="IS6J17Arxvkz"}
## 2.1 Data Preprocessing {#21-data-preprocessing}
:::

::: {.cell .code execution_count="19" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="IiSULvdOxy64" outputId="2732ab5d-8d5c-4952-d680-f938a3e0a722"}
``` python
# counts of unique rows in the 'diagnosis' column
df['diagnosis'].value_counts()
```

::: {.output .execute_result execution_count="19"}
    diagnosis
    B    357
    M    212
    Name: count, dtype: int64
:::
:::

::: {.cell .code execution_count="20" id="vrJEXANzx4D7"}
``` python
# mapping categorical values to numerical values
df['diagnosis']=df['diagnosis'].map({'B':0,'M':1})
```
:::

::: {.cell .code execution_count="21" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="tetSW4P8x4qq" outputId="61a2b9dc-09c6-42a5-b3ea-0b86f0c88e9f"}
``` python
df['diagnosis'].value_counts()
```

::: {.output .execute_result execution_count="21"}
    diagnosis
    0    357
    1    212
    Name: count, dtype: int64
:::
:::

::: {.cell .markdown id="WOynnE1Jx66y"}
## 2.2 Splitting the data into train and test {#22-splitting-the-data-into-train-and-test}
:::

::: {.cell .code execution_count="22" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="iAm6FKTpx9td" outputId="f0fd9c8b-98b3-4431-92ac-1589a10118d7"}
``` python
from sklearn.model_selection import train_test_split

# splitting data
X_train, X_test, y_train, y_test = train_test_split(
                df.drop('diagnosis', axis=1),
                df['diagnosis'],
                test_size=0.2,
                random_state=42)

print("Shape of training set:", X_train.shape)
print("Shape of test set:", X_test.shape)
```

::: {.output .stream .stdout}
    Shape of training set: (455, 30)
    Shape of test set: (114, 30)
:::
:::

::: {.cell .code execution_count="23" id="kSozVIlQyCzg"}
``` python
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
X_train = ss.fit_transform(X_train)
X_test = ss.fit_transform(X_test)
```
:::

::: {.cell .markdown id="hEBkxePLyKWp"}
StandardScaler standardizes a feature by subtracting the mean and then
scaling to unit variance.(Unit variance means dividing all the values by
the standard deviation.)
:::

::: {.cell .markdown id="eBTlAWA3yVKw"}
## 2.3 Classification Models {#23-classification-models}
:::

::: {.cell .markdown id="iqJBJLpKyY7-"}
### 2.3.1 Logistic Regression {#231-logistic-regression}
:::

::: {.cell .code execution_count="24" id="-hUr89dgyXCg"}
``` python
from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression()
logreg.fit(X_train, y_train)
predictions1 = logreg.predict(X_test)
```
:::

::: {.cell .code execution_count="25" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="MuVZnkzVytgW" outputId="fb201fae-c640-43f7-b42b-25f9ce4ed7e4"}
``` python
from sklearn.metrics import confusion_matrix, classification_report

print("Confusion Matrix: \n", confusion_matrix(y_test, predictions1))
print('\n')
print(classification_report(y_test, predictions1))
```

::: {.output .stream .stdout}
    Confusion Matrix: 
     [[71  0]
     [ 2 41]]


                  precision    recall  f1-score   support

               0       0.97      1.00      0.99        71
               1       1.00      0.95      0.98        43

        accuracy                           0.98       114
       macro avg       0.99      0.98      0.98       114
    weighted avg       0.98      0.98      0.98       114
:::
:::

::: {.cell .code execution_count="26" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="xF25F-9uywCC" outputId="405f63ad-fd94-43ad-b811-70a7cde68170"}
``` python
from sklearn.metrics import accuracy_score

logreg_acc = accuracy_score(y_test, predictions1)
print("Accuracy of the Logistic Regression Model is: ", logreg_acc)
```

::: {.output .stream .stdout}
    Accuracy of the Logistic Regression Model is:  0.9824561403508771
:::
:::

::: {.cell .markdown id="8uDpp96Iy0Ho"}
### 2.3.3 Random Forests {#233-random-forests}
:::

::: {.cell .code execution_count="27" id="igfg5WVdyzOH"}
``` python
from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(n_estimators=300)
rfc.fit(X_train, y_train)
predictions4 = rfc.predict(X_test)
```
:::

::: {.cell .code execution_count="28" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="HBvcdPb1y84Q" outputId="e5ebaaa6-a723-4c5a-87d0-85b0129e895d"}
``` python
print("Confusion Matrix: \n", confusion_matrix(y_test, predictions4))
print("\n")
print(classification_report(y_test, predictions4))
```

::: {.output .stream .stdout}
    Confusion Matrix: 
     [[70  1]
     [ 3 40]]


                  precision    recall  f1-score   support

               0       0.96      0.99      0.97        71
               1       0.98      0.93      0.95        43

        accuracy                           0.96       114
       macro avg       0.97      0.96      0.96       114
    weighted avg       0.97      0.96      0.96       114
:::
:::

::: {.cell .code execution_count="29" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="TMcUOTCky-wq" outputId="e13cb768-f373-40d7-eccb-aa30161e0283"}
``` python
rfc_acc = accuracy_score(y_test, predictions4)
print("Accuracy of Random Forests Model is: ", rfc_acc)
```

::: {.output .stream .stdout}
    Accuracy of Random Forests Model is:  0.9649122807017544
:::
:::

::: {.cell .markdown id="SevTl61qzI0q"}
# 3. Final Results {#3-final-results}
:::

::: {.cell .code execution_count="30" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="XWAftzIpzBAc" outputId="2a426522-844f-44c0-da70-c6e6df98dadb"}
``` python
print(logreg_acc)
print(rfc_acc)
```

::: {.output .stream .stdout}
    0.9824561403508771
    0.9649122807017544
:::
:::

::: {.cell .markdown id="5gGWA6qUzbKt"}
The accuracy of Logistic Regression Model is 98.24%

The accuracy of Random Forest Model is 96.49%
:::

::: {.cell .code execution_count="31" colab="{\"base_uri\":\"https://localhost:8080/\",\"height\":539}" id="2O8YUJPIzUWi" outputId="0ed8a641-c30e-4a8a-d8cd-7d5d4ad9e704"}
``` python
plt.figure(figsize=(12,6))
model_acc = [logreg_acc,rfc_acc]
model_name = ['LogisticRegression', 'RandomForests']
sns.barplot(x= model_acc, y=model_name, palette='magma')
```

::: {.output .stream .stderr}
    <ipython-input-31-25f5f1ee76cd>:4: FutureWarning: 

    Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.

      sns.barplot(x= model_acc, y=model_name, palette='magma')
:::

::: {.output .execute_result execution_count="31"}
    <Axes: >
:::

::: {.output .display_data}
![](vertopal_a728d848c81c4bdb9ee2702eb782ef08/c8fc3c4857f3df4ef1dc87240589855c7e32f18f.png)
:::
:::

::: {.cell .code id="2mDKKyRMzxdC"}
``` python
```
:::
